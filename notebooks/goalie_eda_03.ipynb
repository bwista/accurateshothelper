{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Any, Dict, List, Optional, Set, Union\n",
    "import unicodedata\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Configure pandas to display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Add project root to sys.path\n",
    "current_dir = os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from src.entities.lineup import Lineup, Player, Position\n",
    "\n",
    "from src.db.nst_db_utils import *\n",
    "from src.db.nhl_db_utils import get_player_full_name, insert_player_data\n",
    "from src.data_processing.nst_scraper import *\n",
    "from src.data_processing.pbp_utils import *\n",
    "from src.data_processing.game_utils import *\n",
    "from src.data_processing.team_utils import *\n",
    "from src.data_processing.player_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_prefix = 'NHL_DB_'\n",
    "\n",
    "def extract_team_goalies(team: str, reference_date: Optional[str] = None) -> Lineup:\n",
    "    \"\"\"\n",
    "    Gets stats for goalies in the lineup, maintaining lineup order.\n",
    "    \n",
    "    Args:\n",
    "        team (str): The three-letter team code (e.g., 'TOR').\n",
    "        reference_date (Optional[str]): The reference date in 'YYYY-MM-DD' format. Defaults to yesterday's date.\n",
    "    \n",
    "    Returns:\n",
    "        Lineup: A `Lineup` object containing the team's players from the most recent game.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If no recent game is found for the team or if the team is not part of the retrieved game.\n",
    "    \"\"\"\n",
    "    # Step 1: Determine the reference date\n",
    "    if reference_date is None:\n",
    "        today_datetime = datetime.now()\n",
    "        yesterday_datetime = today_datetime - timedelta(days=1, hours=6)  # Adjust for UTC offset if necessary\n",
    "        reference_date = yesterday_datetime.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Step 2: Retrieve the most recent game ID for the team\n",
    "    game_id, back_to_back = get_most_recent_game_id(team, reference_date)\n",
    "    if game_id is None:\n",
    "        raise ValueError(f\"No recent game found for team '{team}' before {reference_date}.\")\n",
    "\n",
    "    # Print the game_id\n",
    "    print(f\"Game ID: {game_id}\")\n",
    "\n",
    "    # Step 3: Fetch the game boxscore data\n",
    "    game_data = get_game_boxscore(game_id, clean=False)\n",
    "\n",
    "    # Step 4: Process the boxscore to obtain skaters and goalies\n",
    "    away_skaters, away_goalies, home_skaters, home_goalies = display_boxscore(game_data)\n",
    "\n",
    "    # Extract team abbreviations to determine if the team is home or away\n",
    "    away_team_code = game_data.get('awayTeam', {}).get('abbrev')\n",
    "    home_team_code = game_data.get('homeTeam', {}).get('abbrev')\n",
    "\n",
    "    if not away_team_code or not home_team_code:\n",
    "        raise ValueError(\"Team abbreviations not found in game data.\")\n",
    "\n",
    "    if team.upper() == away_team_code.upper():\n",
    "        team_side = 'Away'\n",
    "        skaters = away_skaters\n",
    "        goalies = away_goalies\n",
    "    elif team.upper() == home_team_code.upper():\n",
    "        team_side = 'Home'\n",
    "        skaters = home_skaters\n",
    "        goalies = home_goalies\n",
    "    elif team.upper() == 'UTA':\n",
    "        # Since UTA did not exist in the previous season, match with ARI\n",
    "        if 'ARI' == away_team_code.upper():\n",
    "            team_side = 'Away'\n",
    "            skaters = away_skaters\n",
    "            goalies = away_goalies\n",
    "        elif 'ARI' == home_team_code.upper():\n",
    "            team_side = 'Home'\n",
    "            skaters = home_skaters\n",
    "            goalies = home_goalies\n",
    "        else:\n",
    "            raise ValueError(f\"Team '{team}' not found in game ID {game_id}.\")\n",
    "    else:\n",
    "        raise ValueError(f\"Team '{team}' not found in game ID {game_id}.\")\n",
    "\n",
    "    # Step 5: Construct the Lineup object\n",
    "    lineup = Lineup(name=f\"{team.upper()} Lineup from Game {game_id}\")\n",
    "    print(f\"Back to back: {back_to_back}\")\n",
    "    lineup.back_to_back = back_to_back  # Assign back_to_back to the Lineup\n",
    "\n",
    "    # Add Goalies to the Lineup\n",
    "    for _, goalie in goalies.iterrows():\n",
    "        player = Player(\n",
    "            player_id=goalie['playerId'],\n",
    "            name=get_player_full_name(goalie['playerId'], db_prefix, suppress_log=True),\n",
    "            team=team.upper(),\n",
    "            position=Position.G\n",
    "        )\n",
    "        try:\n",
    "            empty_slot = next(i for i, p in enumerate(lineup.goalies) if p is None)\n",
    "            lineup.set_goalie(player, empty_slot)\n",
    "        except StopIteration:\n",
    "            print(f\"No available goalie slot to add player '{player.name}'.\")\n",
    "\n",
    "    return lineup\n",
    "# col_lineup = extract_team_lineup('COL', '2024-12-10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pregame_matchup_stats(input_date: str, team: str, last_n: int=None, team_5v5=None, team_pp=None, team_pk=None, home_away_split: bool=False) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Retrieves and compiles pre-game statistics for both teams in a matchup.\n",
    "    \n",
    "    This function performs the following steps:\n",
    "    1. Calculates reference date and retrieves team statistics\n",
    "    2. Identifies the matchup game and opponent team\n",
    "    3. Compiles team-level statistics for both teams\n",
    "    4. Identifies starting goalies for both teams\n",
    "    \n",
    "    Args:\n",
    "        input_date (str): The game date in 'YYYY-MM-DD' format\n",
    "        team (str): The three-letter team code (e.g., 'TOR')\n",
    "        last_n (int, optional): Number of previous games to consider for team statistics\n",
    "        team_5v5 (dict, optional): Pre-loaded 5v5 team statistics with 'home', 'away', and/or 'both' keys\n",
    "        team_pp (dict, optional): Pre-loaded power play team statistics with 'home', 'away', and/or 'both' keys\n",
    "        team_pk (dict, optional): Pre-loaded penalty kill team statistics with 'home', 'away', and/or 'both' keys\n",
    "        home_away_split (bool, optional): Whether to use home/away split stats (True) or combined stats (False)\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with two rows (one per team) containing:\n",
    "            - Game context (date, game_id, home/away)\n",
    "            - Team statistics (5v5, PP, PK)\n",
    "            - Starting goalie information (name, team, id)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1a: Calculate the date minus one day\n",
    "        reference_datetime = datetime.strptime(input_date, '%Y-%m-%d') - timedelta(days=1)\n",
    "        reference_date_str = reference_datetime.strftime('%Y-%m-%d')\n",
    "        print(f\"Fetching data for reference date: {reference_date_str}\")\n",
    "\n",
    "        # Create a dictionary to store all team stats\n",
    "        team_stats = {}\n",
    "\n",
    "        # Define the stat configurations\n",
    "        stat_configs = [\n",
    "            ('5v5', team_5v5),\n",
    "            ('pp', team_pp),\n",
    "            ('pk', team_pk)\n",
    "        ]\n",
    "\n",
    "        # Fetch any stats that weren't provided\n",
    "        for sit_value, existing_stats in stat_configs:\n",
    "            if existing_stats is None:\n",
    "                if home_away_split:\n",
    "                    team_stats[sit_value] = {\n",
    "                        'away': nst_team_on_ice_scraper(\n",
    "                            startdate='',\n",
    "                            enddate=reference_date_str,\n",
    "                            stype=2,\n",
    "                            sit=sit_value,\n",
    "                            last_n=last_n,\n",
    "                            loc='A'\n",
    "                        ),\n",
    "                        'home': nst_team_on_ice_scraper(\n",
    "                            startdate='',\n",
    "                            enddate=reference_date_str,\n",
    "                            stype=2,\n",
    "                            sit=sit_value,\n",
    "                            last_n=last_n,\n",
    "                            loc='H'\n",
    "                        ),\n",
    "                        'both': pd.DataFrame()\n",
    "                    }\n",
    "                else:\n",
    "                    team_stats[sit_value] = {\n",
    "                        'both': nst_team_on_ice_scraper(\n",
    "                            startdate='',\n",
    "                            enddate=reference_date_str,\n",
    "                            stype=2,\n",
    "                            sit=sit_value,\n",
    "                            last_n=last_n,\n",
    "                            loc='B'\n",
    "                        ),\n",
    "                        'away': pd.DataFrame(),\n",
    "                        'home': pd.DataFrame()\n",
    "                    }\n",
    "            else:\n",
    "                team_stats[sit_value] = existing_stats\n",
    "\n",
    "        # Assign back to original variables\n",
    "        team_5v5 = team_stats['5v5']\n",
    "        team_pp = team_stats['pp']\n",
    "        team_pk = team_stats['pk']\n",
    "\n",
    "        # Step 2: Retrieve matchup games for the input date\n",
    "        print(f\"Retrieving matchup games for date {input_date}.\")\n",
    "        temp_data = get_matchup_games(input_date, input_date)\n",
    "        game_ids = temp_data.get('game_ids', {}).get('id', [])\n",
    "        game_dates = temp_data.get('game_ids', {}).get('date', [])\n",
    "\n",
    "        # Initialize variables\n",
    "        game_id = None\n",
    "        opponent_team_tricode = None\n",
    "        side = None\n",
    "\n",
    "        # Step 3: Identify the game_id involving the specified team\n",
    "        for gid, gdate in zip(game_ids, game_dates):\n",
    "            print(f\"Checking Game ID: {gid} on Date: {gdate}\")\n",
    "            boxscore = get_game_boxscore(gid, clean=True)\n",
    "            away_team = boxscore.get('away_team')\n",
    "            home_team = boxscore.get('home_team')\n",
    "            print(f\"Away Team: {away_team}, Home Team: {home_team}\")\n",
    "            \n",
    "            if team.upper() == away_team.upper():\n",
    "                opponent_team_tricode = home_team.upper()\n",
    "                game_id = gid\n",
    "                side = 'away'  # Team is away, opponent is home\n",
    "                print(f\"Team {team} found as Away Team in Game ID {gid}. Opponent TriCode: {opponent_team_tricode}\")\n",
    "                # Check back-to-back status for both teams\n",
    "                _, team_b2b = get_most_recent_game_id(team, input_date)\n",
    "                _, opponent_b2b = get_most_recent_game_id(opponent_team_tricode, input_date)\n",
    "                break\n",
    "            elif team.upper() == home_team.upper():\n",
    "                opponent_team_tricode = away_team.upper()\n",
    "                game_id = gid\n",
    "                side = 'home'  # Team is home, opponent is away\n",
    "                print(f\"Team {team} found as Home Team in Game ID {gid}. Opponent TriCode: {opponent_team_tricode}\")\n",
    "                # Check back-to-back status for both teams\n",
    "                _, team_b2b = get_most_recent_game_id(team, input_date)\n",
    "                _, opponent_b2b = get_most_recent_game_id(opponent_team_tricode, input_date)\n",
    "                break\n",
    "\n",
    "        if not game_id or not opponent_team_tricode:\n",
    "            raise ValueError(f\"Team {team} did not play on {input_date} or could not determine opponent.\")\n",
    "        \n",
    "        # Step 4: Get full names for both teams\n",
    "        team_fullname = get_fullname_by_tricode(team)\n",
    "        opponent_team_fullname = get_fullname_by_tricode(opponent_team_tricode)\n",
    "\n",
    "        if team_fullname is None or opponent_team_fullname is None:\n",
    "            raise ValueError(f\"Could not find full names for teams: {team} or {opponent_team_tricode}\")\n",
    "        \n",
    "        # Remove accent marks and punctuation from both team names\n",
    "        team_fullname = ''.join(\n",
    "            c for c in unicodedata.normalize('NFD', team_fullname)\n",
    "            if unicodedata.category(c) != 'Mn' and (c.isalnum() or c.isspace())\n",
    "        )\n",
    "        opponent_team_fullname = ''.join(\n",
    "            c for c in unicodedata.normalize('NFD', opponent_team_fullname)\n",
    "            if unicodedata.category(c) != 'Mn' and (c.isalnum() or c.isspace())\n",
    "        )\n",
    "        \n",
    "        # Step 5: Get the appropriate stats based on the side and split type\n",
    "        # For team: use the identified side (home/away) or 'both'\n",
    "        # For opponent: use the opposite side or 'both'\n",
    "        opponent_side = 'home' if side == 'away' else 'away'\n",
    "        \n",
    "        # Determine which dataframes to use based on home_away_split\n",
    "        if home_away_split:\n",
    "            team_df_key = side\n",
    "            opponent_df_key = opponent_side\n",
    "        else:\n",
    "            team_df_key = 'both'\n",
    "            opponent_df_key = 'both'\n",
    "        \n",
    "        # Verify team column exists in all dataframes\n",
    "        if 'team' not in team_5v5[team_df_key].columns:\n",
    "            raise KeyError(f\"Column 'team' not found in team_5v5[{team_df_key}]. Please verify the scraped data.\")\n",
    "        \n",
    "        # Filter 5v5 stats for both teams using the appropriate dataframe\n",
    "        team_5v5_row = team_5v5[team_df_key][team_5v5[team_df_key]['team'].str.lower() == team_fullname.lower()]\n",
    "        opponent_5v5_row = team_5v5[opponent_df_key][team_5v5[opponent_df_key]['team'].str.lower() == opponent_team_fullname.lower()]\n",
    "        \n",
    "        if team_5v5_row.empty or opponent_5v5_row.empty:\n",
    "            raise ValueError(f\"Could not find 5v5 statistics for one or both teams\")\n",
    "        \n",
    "        # Step 6: Create base matchup DataFrame\n",
    "        # First row: Keep team name but use opponent's stats\n",
    "        first_row = opponent_5v5_row.copy()\n",
    "        first_row['team'] = team\n",
    "        \n",
    "        # Second row: Keep opponent name but use team's stats\n",
    "        second_row = team_5v5_row.copy() \n",
    "        second_row['team'] = opponent_team_tricode\n",
    "        \n",
    "        # Create the matchup dataframe with exactly 2 rows\n",
    "        matchup_df = pd.concat([first_row, second_row], ignore_index=True)\n",
    "        \n",
    "        # Step 7: Add PP stats if available\n",
    "        if team_pp is not None and 'team' in team_pp[team_df_key].columns:\n",
    "            # Filter PP stats for both teams using the appropriate dataframe\n",
    "            team_pp_row = team_pp[team_df_key][team_pp[team_df_key]['team'].str.lower() == team_fullname.lower()]\n",
    "            opponent_pp_row = team_pp[opponent_df_key][team_pp[opponent_df_key]['team'].str.lower() == opponent_team_fullname.lower()]\n",
    "            \n",
    "            if not team_pp_row.empty and not opponent_pp_row.empty:\n",
    "                # Add prefix to PP columns to avoid conflicts\n",
    "                pp_cols_to_exclude = ['team', 'gp','w', 'l', 'otl', 'row', 'points', 'point_pct', 'last_game_date', 'season'] # Common columns to exclude from renaming\n",
    "                \n",
    "                # Create new column names with 'pp_' prefix\n",
    "                pp_renamed_cols = {}\n",
    "                for col in team_pp_row.columns:\n",
    "                    if col not in pp_cols_to_exclude:\n",
    "                        pp_renamed_cols[col] = f'pp_{col}'\n",
    "                \n",
    "                # Create copies with renamed columns\n",
    "                team_pp_renamed = team_pp_row.copy().rename(columns=pp_renamed_cols)\n",
    "                opponent_pp_renamed = opponent_pp_row.copy().rename(columns=pp_renamed_cols)\n",
    "                \n",
    "                # Create a DataFrame with all PP data\n",
    "                pp_data = pd.DataFrame(index=[0, 1])\n",
    "                for col in pp_renamed_cols.values():\n",
    "                    if col in team_pp_renamed.columns:\n",
    "                        # CHANGE: Swap the assignment - opponent's PP stats in first row, team's PP stats in second row\n",
    "                        pp_data.loc[0, col] = opponent_pp_renamed[col].values[0]  # Changed from team_pp_renamed\n",
    "                        pp_data.loc[1, col] = team_pp_renamed[col].values[0]      # Changed from opponent_pp_renamed\n",
    "                \n",
    "                # Join PP data to matchup_df\n",
    "                matchup_df = pd.concat([matchup_df, pp_data], axis=1)\n",
    "        \n",
    "        # Step 8: Add PK stats if available\n",
    "        if team_pk is not None and 'team' in team_pk[team_df_key].columns:\n",
    "            # Filter PK stats for both teams using the appropriate dataframe\n",
    "            team_pk_row = team_pk[team_df_key][team_pk[team_df_key]['team'].str.lower() == team_fullname.lower()]\n",
    "            opponent_pk_row = team_pk[opponent_df_key][team_pk[opponent_df_key]['team'].str.lower() == opponent_team_fullname.lower()]\n",
    "            \n",
    "            if not team_pk_row.empty and not opponent_pk_row.empty:\n",
    "                # Add prefix to PK columns to avoid conflicts\n",
    "                pk_cols_to_exclude = ['team', 'gp', 'w', 'l', 'otl', 'row', 'points', 'point_pct', 'last_game_date', 'season']  # Common columns to exclude from renaming\n",
    "                \n",
    "                # Create new column names with 'pk_' prefix\n",
    "                pk_renamed_cols = {}\n",
    "                for col in team_pk_row.columns:\n",
    "                    if col not in pk_cols_to_exclude:\n",
    "                        pk_renamed_cols[col] = f'pk_{col}'\n",
    "                \n",
    "                # Create copies with renamed columns\n",
    "                team_pk_renamed = team_pk_row.copy().rename(columns=pk_renamed_cols)\n",
    "                opponent_pk_renamed = opponent_pk_row.copy().rename(columns=pk_renamed_cols)\n",
    "                \n",
    "                # Create a DataFrame with all PK data\n",
    "                pk_data = pd.DataFrame(index=[0, 1])\n",
    "                for col in pk_renamed_cols.values():\n",
    "                    if col in team_pk_renamed.columns:\n",
    "                        # CHANGE: Swap the assignment - opponent's PK stats in first row, team's PK stats in second row\n",
    "                        pk_data.loc[0, col] = opponent_pk_renamed[col].values[0]  # Changed from team_pk_renamed\n",
    "                        pk_data.loc[1, col] = team_pk_renamed[col].values[0]      # Changed from opponent_pk_renamed\n",
    "                \n",
    "                # Join PK data to matchup_df\n",
    "                matchup_df = pd.concat([matchup_df, pk_data], axis=1)\n",
    "        \n",
    "        # Step 9: Add additional context columns\n",
    "        matchup_df['home'] = [side == 'home', side == 'away']  # Updated to use the new side variable\n",
    "        matchup_df['game_id'] = game_id\n",
    "        matchup_df['game_date'] = input_date\n",
    "        matchup_df['b2b'] = [team_b2b, opponent_b2b]  # First row has team's b2b, second row has opponent's b2b\n",
    "        matchup_df['opp_b2b'] = [opponent_b2b, team_b2b]  # First row has opponent's b2b, second row has team's b2b\n",
    "\n",
    "        # Ensure team names are correct\n",
    "        matchup_df.loc[0, 'team'] = team\n",
    "        matchup_df.loc[1, 'team'] = opponent_team_tricode\n",
    "\n",
    "        # Step 10: Drop specified columns\n",
    "        columns_to_drop = ['w', 'l', 'otl', 'row', 'points', 'point_pct'] #'gp', 'toi', added back for feature extraction\n",
    "        matchup_df = matchup_df.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "        # Step 11: Reorder columns to put game_date and game_id first, and move last_game_date and season after opp_b2b\n",
    "        cols = matchup_df.columns.tolist()\n",
    "        \n",
    "        # First remove all the columns we want to reorder\n",
    "        for col in ['game_date', 'game_id', 'home', 'last_game_date', 'season']:\n",
    "            if col in cols:\n",
    "                cols.remove(col)\n",
    "        \n",
    "        # Then insert them in the desired order\n",
    "        cols = ['game_date', 'game_id', 'home'] + cols\n",
    "        \n",
    "        # Find the position of opp_b2b to insert last_game_date and season after it\n",
    "        if 'opp_b2b' in cols:\n",
    "            opp_b2b_pos = cols.index('opp_b2b')\n",
    "            if 'last_game_date' in matchup_df.columns:\n",
    "                cols.insert(opp_b2b_pos + 1, 'last_game_date')\n",
    "            if 'season' in matchup_df.columns:\n",
    "                cols.insert(opp_b2b_pos + 2 if 'last_game_date' in matchup_df.columns else opp_b2b_pos + 1, 'season')\n",
    "        \n",
    "        # Apply the new column order\n",
    "        matchup_df = matchup_df[cols]\n",
    "        \n",
    "        # Step 12: Get goalie information for both teams\n",
    "        goalie_info = []\n",
    "        for idx, row in matchup_df.iterrows():\n",
    "            try:\n",
    "                # If the game is in the past, just read the goalie from the boxscore\n",
    "                if input_date < datetime.now().strftime('%Y-%m-%d'):\n",
    "                    boxscore = get_game_boxscore(row['game_id'], clean=False)\n",
    "                    # Use the correct team type based on home/away status\n",
    "                    team_type = 'awayTeam' if not row['home'] else 'homeTeam'\n",
    "                    goalies = boxscore['playerByGameStats'][team_type]['goalies']\n",
    "                    \n",
    "                    if goalies:\n",
    "                        # Sort goalies by TOI (descending) and take the one with most ice time\n",
    "                        starting_goalie = max(goalies, key=lambda x: x.get('toi', '00:00'))\n",
    "                        if starting_goalie.get('toi') != '00:00':\n",
    "                            goalie_name = get_player_full_name(starting_goalie.get('playerId'), 'NHL_DB_', suppress_log=True)\n",
    "                            if goalie_name is None:\n",
    "                                player_data = fetch_player_data(starting_goalie.get('playerId'))\n",
    "                                if player_data:\n",
    "                                    insert_player_data(player_data, 'NHL_DB_')\n",
    "                                goalie_name = get_player_full_name(starting_goalie.get('playerId'), 'NHL_DB_', suppress_log=True)\n",
    "                            goalie_info.append({\n",
    "                                'goalie_name': goalie_name,\n",
    "                                'goalie_team': row['team'],\n",
    "                                'goalie_id': starting_goalie.get('playerId')\n",
    "                            })\n",
    "                        else:\n",
    "                            goalie_info.append({\n",
    "                                'goalie_name': None,\n",
    "                                'goalie_team': None,\n",
    "                                'goalie_id': None\n",
    "                            })\n",
    "                    else:\n",
    "                        goalie_info.append({\n",
    "                            'goalie_name': None,\n",
    "                            'goalie_team': None,\n",
    "                            'goalie_id': None\n",
    "                        })\n",
    "                else:\n",
    "                    lineup = extract_team_goalies(row['team'], input_date)\n",
    "                    if lineup.goalies[0] is not None:  # Get starting goalie\n",
    "                        goalie = lineup.goalies[0]\n",
    "                        goalie_info.append({\n",
    "                            'goalie_name': goalie.name,\n",
    "                            'goalie_team': goalie.team,\n",
    "                            'goalie_id': goalie.player_id\n",
    "                        })\n",
    "                    else:\n",
    "                        goalie_info.append({\n",
    "                            'goalie_name': None,\n",
    "                            'goalie_team': None,\n",
    "                            'goalie_id': None\n",
    "                        })\n",
    "            except Exception as e:\n",
    "                print(f\"Error getting goalie for {row['team']}: {e}\")\n",
    "                goalie_info.append({\n",
    "                    'goalie_name': None,\n",
    "                    'goalie_team': None,\n",
    "                    'goalie_id': None\n",
    "                })\n",
    "        \n",
    "        # Step 13: Add goalie information to matchup_df\n",
    "        goalie_data = pd.DataFrame(goalie_info)\n",
    "        if not goalie_data.empty:\n",
    "            for col in goalie_data.columns:\n",
    "                matchup_df[col] = goalie_data[col].values\n",
    "        \n",
    "        # Step 14: Ensure we only have 2 rows\n",
    "        if len(matchup_df) > 2:\n",
    "            print(f\"Warning: Matchup dataframe has {len(matchup_df)} rows, expected 2. Keeping only the first 2 rows.\")\n",
    "            matchup_df = matchup_df.iloc[:2]\n",
    "        \n",
    "        # Step 15: Create a clean copy to defragment the DataFrame\n",
    "        matchup_df = matchup_df.copy()\n",
    "        \n",
    "        return matchup_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during processing: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # Example usage of the process_team_and_opponent function\n",
    "# input_date = '2025-02-08'\n",
    "# team = 'CHI' \n",
    "# last_n = 10\n",
    "# team_5v5 = {\n",
    "#     'away': get_team_stats(end_date=input_date, last_n=last_n, db_prefix=\"NST_DB_\", situation=\"5v5\", stype=2, side=\"away\"),\n",
    "#     'home': get_team_stats(end_date=input_date, last_n=last_n, db_prefix=\"NST_DB_\", situation=\"5v5\", stype=2, side=\"home\")\n",
    "# }\n",
    "# team_pk = {\n",
    "#     'away': get_team_stats(end_date=input_date, last_n=last_n, db_prefix=\"NST_DB_\", situation=\"pk\", stype=2, side=\"away\"),\n",
    "#     'home': get_team_stats(end_date=input_date, last_n=last_n, db_prefix=\"NST_DB_\", situation=\"pk\", stype=2, side=\"home\")\n",
    "# }\n",
    "# team_pp = {\n",
    "#     'away': get_team_stats(end_date=input_date, last_n=last_n, db_prefix=\"NST_DB_\", situation=\"pp\", stype=2, side=\"away\"),\n",
    "#     'home': get_team_stats(end_date=input_date, last_n=last_n, db_prefix=\"NST_DB_\", situation=\"pp\", stype=2, side=\"home\")\n",
    "# }\n",
    "# matchup_data = get_pregame_matchup_stats(input_date, team, last_n=last_n, team_5v5=team_5v5, team_pk=team_pk, team_pp=team_pp)\n",
    "# matchup_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_goalie_rolling_stats(df, player_name: str, window_size: int = 10, date: str = None):\n",
    "    \"\"\"Create rolling averages and statistics for a specific goalie\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe with goalie statistics\n",
    "        player_name (str): Name of the player to process\n",
    "        window_size (int, optional): Size of rolling window for statistics. Defaults to 10.\n",
    "        date (str, optional): If provided, filter out data on or after this date\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Processed statistics for the specified player or league averages if player not found\n",
    "    \"\"\"\n",
    "    # Make a copy and filter for specific player\n",
    "    player_df = df[df['player'] == player_name].copy()\n",
    "    \n",
    "    # If no data found for player, use league averages\n",
    "    if player_df.empty:\n",
    "        print(f\"No data found for player: {player_name}. Using league averages.\")\n",
    "        \n",
    "        # Calculate league averages\n",
    "        league_df = df.copy()\n",
    "        league_df['date'] = pd.to_datetime(league_df['date'])\n",
    "        \n",
    "        if date:\n",
    "            cutoff_date = pd.to_datetime(date)\n",
    "            league_df = league_df[league_df['date'] < cutoff_date]\n",
    "        \n",
    "        # Convert numeric columns from object to float\n",
    "        numeric_cols = ['sv_pct', 'gaa', 'gsaa', 'xg_against', 'hdsv_pct', \n",
    "                       'mdsv_pct', 'ldsv_pct', 'avg_shot_distance', 'avg_goal_distance']\n",
    "        for col in numeric_cols:\n",
    "            league_df[col] = pd.to_numeric(league_df[col], errors='coerce')\n",
    "        \n",
    "        # Calculate league averages for all relevant columns\n",
    "        feature_columns = [\n",
    "            'sa', 'sv_pct', 'gaa', 'gsaa',\n",
    "            'xg_against', \n",
    "            'hd_sa', 'hdsv_pct',\n",
    "            'md_sa', 'mdsv_pct',\n",
    "            'ld_sa', 'ldsv_pct'\n",
    "        ]\n",
    "        \n",
    "        # Map original column names to shortened versions\n",
    "        col_mapping = {\n",
    "            'shots_against': 'sa',\n",
    "            'hd_shots_against': 'hd_sa',\n",
    "            'md_shots_against': 'md_sa',\n",
    "            'ld_shots_against': 'ld_sa'\n",
    "        }\n",
    "        \n",
    "        # Create a single row DataFrame with league averages\n",
    "        league_averages = pd.DataFrame([{\n",
    "            f'{col}_roll_avg': league_df[col_mapping.get(col, col)].mean() for col in feature_columns\n",
    "        }])\n",
    "        \n",
    "        # Add standard deviations\n",
    "        for col in feature_columns:\n",
    "            league_averages[f'{col}_roll_std'] = league_df[col_mapping.get(col, col)].std()\n",
    "        \n",
    "        # Add workload features (use median values) with shortened names\n",
    "        league_averages['rest'] = 3.0  # typical rest between games\n",
    "        league_averages['l7'] = 2.0  # typical games in 7 days\n",
    "        \n",
    "        return league_averages\n",
    "    \n",
    "    # If we have player data, proceed with normal calculations\n",
    "    player_df['date'] = pd.to_datetime(player_df['date'])\n",
    "    player_df = player_df.sort_values('date')\n",
    "    \n",
    "    # Convert numeric columns from object to float\n",
    "    numeric_cols = ['sv_pct', 'gaa', 'gsaa', 'xg_against', 'hdsv_pct', \n",
    "                   'mdsv_pct', 'ldsv_pct', 'avg_shot_distance', 'avg_goal_distance']\n",
    "    for col in numeric_cols:\n",
    "        player_df[col] = pd.to_numeric(player_df[col], errors='coerce')\n",
    "    \n",
    "    # Rename columns with shortened names\n",
    "    rename_dict = {\n",
    "        'shots_against': 'sa',\n",
    "        'hd_shots_against': 'hd_sa',\n",
    "        'md_shots_against': 'md_sa',\n",
    "        'ld_shots_against': 'ld_sa'\n",
    "    }\n",
    "    player_df = player_df.rename(columns=rename_dict)\n",
    "    \n",
    "    feature_columns = [\n",
    "        'sa', 'sv_pct', 'gaa', 'gsaa',\n",
    "        'xg_against', \n",
    "        'hd_sa', 'hdsv_pct',\n",
    "        'md_sa', 'mdsv_pct',\n",
    "        'ld_sa', 'ldsv_pct'\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        print(f\"Processing player: {player_name}\")\n",
    "        print(f\"Data shape: {player_df.shape}\")\n",
    "        \n",
    "        # Calculate rolling statistics with updated names\n",
    "        for col in feature_columns:\n",
    "            player_df[f'{col}_roll_avg'] = player_df[col].rolling(\n",
    "                window=window_size, min_periods=1\n",
    "            ).mean()\n",
    "            player_df[f'{col}_roll_std'] = player_df[col].rolling(\n",
    "                window=window_size, min_periods=1\n",
    "            ).std()\n",
    "        \n",
    "        # Add workload features with shortened names\n",
    "        player_df['rest'] = player_df['date'].diff().dt.days\n",
    "        player_df['l7'] = player_df.rolling('7D', on='date')['date'].count()\n",
    "        \n",
    "        print(f\"Successfully processed {player_name}\")\n",
    "        \n",
    "        # Filter out dates if date is provided\n",
    "        if date:\n",
    "            cutoff_date = pd.to_datetime(date)\n",
    "            player_df = player_df[player_df['date'] < cutoff_date]\n",
    "            if player_df.empty:\n",
    "                print(f\"No data found for {player_name} before {date}. Using league averages.\")\n",
    "                return calculate_goalie_rolling_stats(df, player_name, window_size)  # Recursive call without date\n",
    "            player_df = player_df.tail(1).reset_index(drop=True)\n",
    "\n",
    "        # Keep only date, rolling features, and workload features\n",
    "        rolling_cols = [col for col in player_df.columns if '_roll_' in col]\n",
    "        cols_to_keep = ['date', 'player', 'team'] + rolling_cols + ['rest', 'l7']\n",
    "        player_df = player_df[cols_to_keep]\n",
    "            \n",
    "        return player_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing player {player_name}: {str(e)}\")\n",
    "        print(\"Data types:\", player_df.dtypes)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_matchup_with_goalie_stats(\n",
    "    matchup_df: pd.DataFrame,\n",
    "    goalie_stats_5v5: pd.DataFrame,\n",
    "    goalie_stats_pk: pd.DataFrame,\n",
    "    window_size: int = 10,\n",
    "    home_away_split: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Enriches matchup data with rolling goalie statistics.\n",
    "    \n",
    "    Args:\n",
    "        matchup_df (pd.DataFrame): Output from get_pregame_matchup_stats\n",
    "        goalie_stats_df (pd.DataFrame): Raw goalie statistics\n",
    "        window_size (int): Window size for rolling calculations\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Enhanced matchup data with goalie rolling statistics\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying original\n",
    "    enriched_df = matchup_df.copy()\n",
    "    \n",
    "    # Process each goalie in the matchup\n",
    "    for idx, row in enriched_df.iterrows():\n",
    "        if pd.isna(row['goalie_name']):\n",
    "            continue\n",
    "        if row['home'] == True:\n",
    "            if home_away_split:\n",
    "                side = 'home'\n",
    "            else:\n",
    "                side = 'both'\n",
    "        else:  \n",
    "            if home_away_split:\n",
    "                side = 'away'\n",
    "            else:\n",
    "                side = 'both'\n",
    "        try:\n",
    "            # Calculate rolling stats for this goalie, fetching the appropriate side (home or away)\n",
    "            goalie_rolling_5v5 = calculate_goalie_rolling_stats(\n",
    "                goalie_stats_5v5[side],\n",
    "                player_name=row['goalie_name'],\n",
    "                window_size=window_size,\n",
    "                date=row['game_date']\n",
    "            )\n",
    "            \n",
    "            goalie_rolling_pk = calculate_goalie_rolling_stats(\n",
    "                goalie_stats_pk[side],\n",
    "                player_name=row['goalie_name'],\n",
    "                window_size=window_size,\n",
    "                date=row['game_date']\n",
    "            )\n",
    "            \n",
    "            \n",
    "            if not goalie_rolling_5v5.empty and not goalie_rolling_pk.empty:\n",
    "                # Add prefix to rolling columns to avoid confusion with team stats\n",
    "                rolling_cols_5v5 = [col for col in goalie_rolling_5v5.columns \n",
    "                                  if col not in ['date', 'player', 'team']]\n",
    "                rolling_cols_pk = [col for col in goalie_rolling_pk.columns\n",
    "                                 if col not in ['date', 'player', 'team']]\n",
    "                \n",
    "                # Add 5v5 stats, assume that no prefix is 5v5 like team stats\n",
    "                for col in rolling_cols_5v5:\n",
    "                    enriched_df.at[idx, f'g_{col}'] = goalie_rolling_5v5.iloc[0][col]\n",
    "                \n",
    "                # Add PK stats    \n",
    "                for col in rolling_cols_pk:\n",
    "                    # Skip rest and l7 columns as they're redundant\n",
    "                    if col not in ['rest', 'l7']:\n",
    "                        enriched_df.at[idx, f'g_pk_{col}'] = goalie_rolling_pk.iloc[0][col]\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing goalie {row['goalie_name']}: {e}\")\n",
    "            # Continue with next goalie if one fails\n",
    "            continue\n",
    "    \n",
    "    return enriched_df\n",
    "\n",
    "# matchup_data = enrich_matchup_with_goalie_stats(matchup_data, goalie_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matchup_goalie_results(matchup_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieves actual game results for goalies from a matchup DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        matchup_df (pd.DataFrame): DataFrame containing matchup data with game_id column\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Original matchup data with additional columns for actual game results\n",
    "    \"\"\"\n",
    "    # Get the boxscore data\n",
    "    game_id = matchup_df['game_id'].iloc[0]\n",
    "    game_data = get_game_boxscore(game_id, clean=False)\n",
    "    player_stats = game_data.get('playerByGameStats', {})\n",
    "    \n",
    "    # Create a copy of the input DataFrame\n",
    "    enriched_df = matchup_df.copy()\n",
    "    \n",
    "    # Process each team's data\n",
    "    for idx, row in enriched_df.iterrows():\n",
    "        team_type = 'homeTeam' if row['home'] else 'awayTeam'\n",
    "        goalies = player_stats.get(team_type, {}).get('goalies', [])\n",
    "        \n",
    "        if goalies:\n",
    "            # Find the goalie whose name matches the one in enriched_df\n",
    "            matching_goalie = None\n",
    "            for g in goalies:\n",
    "                goalie_name = get_player_full_name(g.get('playerId'), 'NHL_DB_', suppress_log=True)\n",
    "                if goalie_name == row['goalie_name']:\n",
    "                    matching_goalie = g\n",
    "                    break\n",
    "            \n",
    "            # If no matching goalie found, use the first one (as before)\n",
    "            goalie = matching_goalie or goalies[0]\n",
    "            \n",
    "            shots = goalie.get('shotsAgainst', 0)\n",
    "            saves = goalie.get('saves', 0)\n",
    "            \n",
    "            enriched_df.loc[idx, 'res_sv'] = saves\n",
    "            enriched_df.loc[idx, 'res_sa'] = shots\n",
    "            enriched_df.loc[idx, 'res_sv_pct'] = round(saves / shots if shots > 0 else 0.0, 3)\n",
    "            enriched_df.loc[idx, 'res_ga'] = goalie.get('goalsAgainst', 0)\n",
    "            enriched_df.loc[idx, 'res_des'] = goalie.get('decision', 'N/A')\n",
    "            enriched_df.loc[idx, 'res_toi'] = goalie.get('toi', '00:00')  # Add time on ice\n",
    "    \n",
    "    return enriched_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_matchups_for_date(input_date: str, last_n: int = None, home_away_split: bool=False ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Processes all matchup games for a given date by getting matchup stats, \n",
    "    enriching with goalie stats, and getting actual game results.\n",
    "    \n",
    "    Args:\n",
    "        input_date (str): The reference date in 'YYYY-MM-DD' format\n",
    "        last_n (int, optional): Number of last games to consider for stats\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing all matchups for the date with \n",
    "                     pre-game stats and actual results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: Get matchup games for the date\n",
    "        temp_data = get_matchup_games(input_date, input_date)\n",
    "        game_ids = temp_data.get('game_ids', {}).get('id', [])\n",
    "        \n",
    "        if not game_ids:\n",
    "            print(f\"No games found for the date {input_date}.\")\n",
    "            return pd.DataFrame()\n",
    "       \n",
    "        # Parse the input date and subtract one day\n",
    "        day_before = (datetime.strptime(input_date, '%Y-%m-%d') - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "        \n",
    "        # print(f\"Input date: {input_date}, Day before: {day_before}\")\n",
    "\n",
    "        # Step 2: Get goalie stats for enrichment\n",
    "        # This creates a nested dictionary structure where:\n",
    "        # - The outer key is the stat_type ('5v5' or 'pk')\n",
    "        # - The inner key is the side ('both', 'away', or 'home')\n",
    "        # - The value is the DataFrame of goalie stats for that combination\n",
    "        goalie_stats = {}\n",
    "        for stat_type, situation in [('5v5', '5v5'), ('pk', 'pk')]:\n",
    "            if home_away_split:\n",
    "                goalie_stats[stat_type] = {\n",
    "                    'both': pd.DataFrame(),\n",
    "                    'away': get_goalie_stats(\n",
    "                        end_date=day_before,\n",
    "                        situation=situation,\n",
    "                        side='away'\n",
    "                    ),\n",
    "                    'home': get_goalie_stats(\n",
    "                        end_date=day_before,\n",
    "                        situation=situation,\n",
    "                        side='home'\n",
    "                    )\n",
    "                }\n",
    "            else:\n",
    "                goalie_stats[stat_type] = {\n",
    "                    'both': get_goalie_stats(\n",
    "                        end_date=day_before,\n",
    "                        situation=situation,\n",
    "                        side=None\n",
    "                    ),\n",
    "                    'away': pd.DataFrame(),\n",
    "                    'home': pd.DataFrame()\n",
    "                }\n",
    "\n",
    "        # Step 3: Try to get team stats from database first, fall back to scraper if needed\n",
    "        # This creates a nested dictionary structure where:\n",
    "        # - The outer key is the situation ('5v5', 'pp', or 'pk')\n",
    "        # - The inner key is the side ('both', 'away', or 'home')\n",
    "        # - The value is the DataFrame of team stats for that combination\n",
    "        team_stats = {}\n",
    "        try:\n",
    "            print(f\"Attempting to retrieve team stats from database for date: {day_before}\")\n",
    "            \n",
    "            # Get team stats for each situation and side from database\n",
    "            for situation in ['5v5', 'pp', 'pk']:\n",
    "                if home_away_split:\n",
    "                    team_stats[situation] = {\n",
    "                        'both': pd.DataFrame(),\n",
    "                        'away': get_team_stats(\n",
    "                            end_date=day_before,\n",
    "                            last_n=last_n,\n",
    "                            db_prefix=\"NST_DB_\",\n",
    "                            situation=situation,\n",
    "                            stype=2,\n",
    "                            side='away'\n",
    "                        ),\n",
    "                        'home': get_team_stats(\n",
    "                            end_date=day_before,\n",
    "                            last_n=last_n,\n",
    "                            db_prefix=\"NST_DB_\", \n",
    "                            situation=situation,\n",
    "                            stype=2,\n",
    "                            side='home'\n",
    "                        )\n",
    "                    }\n",
    "                else:\n",
    "                    team_stats[situation] = {\n",
    "                        'both': get_team_stats(\n",
    "                            end_date=day_before,\n",
    "                            last_n=last_n,\n",
    "                            db_prefix=\"NST_DB_\",\n",
    "                            situation=situation, \n",
    "                            stype=2,\n",
    "                            side=None\n",
    "                        ),\n",
    "                        'away': pd.DataFrame(),\n",
    "                        'home': pd.DataFrame()\n",
    "                    }\n",
    "            \n",
    "            print(f\"Successfully retrieved all team stats from database.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not retrieve team stats from database: {e}\")\n",
    "            print(f\"Falling back to scraper to get team stats...\")\n",
    "            \n",
    "            # Fall back to scraper for each situation and side\n",
    "            for situation in ['5v5', 'pp', 'pk']:\n",
    "                if home_away_split:\n",
    "                    team_stats[situation] = {\n",
    "                        'both': pd.DataFrame(),\n",
    "                        'away': nst_team_on_ice_scraper(\n",
    "                            startdate='',\n",
    "                            enddate=day_before,\n",
    "                            stype=2,\n",
    "                            sit=situation,\n",
    "                            last_n=last_n,\n",
    "                            loc='A'\n",
    "                        ),\n",
    "                        'home': nst_team_on_ice_scraper(\n",
    "                            startdate='',\n",
    "                            enddate=day_before,\n",
    "                            stype=2,\n",
    "                            sit=situation,\n",
    "                            last_n=last_n,\n",
    "                            loc='H'\n",
    "                        )\n",
    "                    }\n",
    "                else:\n",
    "                    team_stats[situation] = {\n",
    "                        'both': nst_team_on_ice_scraper(\n",
    "                            startdate='',\n",
    "                            enddate=day_before,\n",
    "                            stype=2,\n",
    "                            sit=situation,\n",
    "                            last_n=last_n,\n",
    "                            loc='B'\n",
    "                        ),\n",
    "                        'away': pd.DataFrame(),\n",
    "                        'home': pd.DataFrame()\n",
    "                    }\n",
    "\n",
    "        results = []\n",
    "        \n",
    "        # Step 4: Process each game\n",
    "        for game_id in game_ids:\n",
    "            print(f\"\\nProcessing Game ID: {game_id}\")\n",
    "            \n",
    "            # Get boxscore to determine teams\n",
    "            boxscore = get_game_boxscore(game_id, clean=True)\n",
    "            away_team = boxscore.get('away_team')\n",
    "            home_team = boxscore.get('home_team')\n",
    "            \n",
    "            if not away_team or not home_team:\n",
    "                print(f\"Could not extract teams for Game ID: {game_id}. Skipping.\")\n",
    "                continue\n",
    "                \n",
    "            # Process teams (both home and away)\n",
    "            try:\n",
    "                matchup = get_pregame_matchup_stats(input_date, away_team, last_n=last_n, team_5v5=team_stats['5v5'], team_pp=team_stats['pp'], team_pk=team_stats['pk'], home_away_split=home_away_split)\n",
    "                matchup = enrich_matchup_with_goalie_stats(matchup, goalie_stats['5v5'], goalie_stats['pk'], window_size=last_n, home_away_split=home_away_split)\n",
    "                matchup = get_matchup_goalie_results(matchup)\n",
    "                results.append(matchup)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing away team {away_team}: {e}\")\n",
    "\n",
    "        # Combine all results\n",
    "        if results:\n",
    "            final_df = pd.concat(results, ignore_index=True)\n",
    "            print(\"\\nAll matchups processed successfully.\")\n",
    "            return final_df\n",
    "        else:\n",
    "            print(\"No matchups were successfully processed.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during processing: {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchups = process_matchups_for_date('2024-12-11', last_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_matchups_for_date_range(start_date: str, end_date: str, last_n: int = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Processes matchup games for a range of dates by calling process_matchups_for_date for each date.\n",
    "    \n",
    "    Args:\n",
    "        start_date (str): Start date in 'YYYY-MM-DD' format\n",
    "        end_date (str): End date in 'YYYY-MM-DD' format\n",
    "        last_n (int, optional): Number of last games to consider for stats\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing all matchups for the date range with \n",
    "                     pre-game stats and actual results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert dates to datetime objects\n",
    "        start = pd.to_datetime(start_date)\n",
    "        end = pd.to_datetime(end_date)\n",
    "        \n",
    "        # Generate list of dates\n",
    "        dates = pd.date_range(start=start, end=end, freq='D')\n",
    "        \n",
    "        all_results = []\n",
    "        \n",
    "        # Process each date\n",
    "        for date in dates:\n",
    "            date_str = date.strftime('%Y-%m-%d')\n",
    "            print(f\"\\nProcessing date: {date_str}\")\n",
    "            \n",
    "            # Process matchups for this date\n",
    "            daily_results = process_matchups_for_date(date_str, last_n=last_n)\n",
    "            if not daily_results.empty:\n",
    "                all_results.append(daily_results)\n",
    "\n",
    "            time.sleep(random.randint(1, 2))\n",
    "        \n",
    "        # Combine all results\n",
    "        if all_results:\n",
    "            final_df = pd.concat(all_results, ignore_index=True)\n",
    "            print(f\"\\nProcessed {len(dates)} days of matchups successfully.\")\n",
    "            return final_df\n",
    "        else:\n",
    "            print(\"No matchups were successfully processed for the date range.\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during processing: {e}\")\n",
    "        raise\n",
    "\n",
    "# Example usage:\n",
    "# matchups_range = process_matchups_for_date_range('2023-01-01', '2023-01-31', last_n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matchups_range.to_csv('../data/g_15_01_23.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_processing.season_utils import get_season_end_date, get_season_start_date\n",
    "import calendar\n",
    "\n",
    "def process_season_by_month(year, last_n=15):\n",
    "    \"\"\"\n",
    "    Process an entire NHL season by month and save each month's data to a separate CSV file.\n",
    "    \n",
    "    Args:\n",
    "        year (int): The starting year of the season (e.g., 2021 for the 2021-2022 season)\n",
    "        last_n (int): Number of previous games to consider for rolling stats\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with month names as keys and DataFrames as values\n",
    "    \"\"\"\n",
    "    season = int(f\"{year}{year+1}\")\n",
    "    \n",
    "    # Determine season start and end dates\n",
    "    if year >= 2021:\n",
    "        # Regular seasons since 2021-2022\n",
    "        start_date = f\"{year}-10-01\"  # Approximate start in October\n",
    "    elif year == 2020:\n",
    "        # COVID-shortened 2020-2021 season\n",
    "        start_date = \"2021-01-13\"\n",
    "    else:\n",
    "        start_date = f\"{year}-10-01\"  # Default for earlier seasons\n",
    "    \n",
    "    try:\n",
    "        # Try to get the official end date from season_utils\n",
    "        end_date = get_season_end_date(season, stype=2)  # Regular season end\n",
    "    except ValueError:\n",
    "        # If not available, use a reasonable default (end of April next year)\n",
    "        end_date = f\"{year+1}-04-30\"\n",
    "    \n",
    "    print(f\"Processing {year}-{year+1} NHL season from {start_date} to {end_date}\")\n",
    "    \n",
    "    # Convert dates to datetime for easier manipulation\n",
    "    start = pd.to_datetime(start_date)\n",
    "    end = pd.to_datetime(end_date)\n",
    "    \n",
    "    # Process each month in the season\n",
    "    current = start\n",
    "    results = {}\n",
    "    \n",
    "    while current <= end:\n",
    "        year_month = current.strftime('%Y-%m')\n",
    "        month_name = current.strftime('%b').lower()\n",
    "        year_short = current.strftime('%y')\n",
    "        \n",
    "        # Get the last day of the current month\n",
    "        if current.month == 12:\n",
    "            last_day = 31\n",
    "        else:\n",
    "            last_day = calendar.monthrange(current.year, current.month)[1]\n",
    "        \n",
    "        month_start = f\"{year_month}-01\"\n",
    "        month_end = f\"{year_month}-{last_day}\"\n",
    "        \n",
    "        # Adjust if this is the start or end of the season\n",
    "        if pd.to_datetime(month_start) < start:\n",
    "            month_start = start.strftime('%Y-%m-%d')\n",
    "        if pd.to_datetime(month_end) > end:\n",
    "            month_end = end.strftime('%Y-%m-%d')\n",
    "        \n",
    "        print(f\"\\nProcessing month: {month_name.upper()} {current.year}\")\n",
    "        \n",
    "        # Process the month\n",
    "        try:\n",
    "            monthly_data = process_matchups_for_date_range(month_start, month_end, last_n=last_n)\n",
    "            \n",
    "            if not monthly_data.empty:\n",
    "                # Save to CSV\n",
    "                filename = f\"../data/g_{last_n}-1_{month_name}_{year_short}.csv\"\n",
    "                monthly_data.to_csv(filename, index=False)\n",
    "                print(f\"Saved {len(monthly_data)} matchups to {filename}\")\n",
    "                results[f\"{month_name}_{year_short}\"] = monthly_data\n",
    "            else:\n",
    "                print(f\"No data found for {month_name.upper()} {current.year}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {month_name.upper()} {current.year}: {e}\")\n",
    "        \n",
    "        # Move to next month\n",
    "        if current.month == 12:\n",
    "            current = pd.Timestamp(year=current.year + 1, month=1, day=1)\n",
    "        else:\n",
    "            current = pd.Timestamp(year=current.year, month=current.month + 1, day=1)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "# season_2022_2023 = process_season_by_month(2022, last_n=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_2023_2024 = process_season_by_month(2023, last_n=7)\n",
    "season_2022_2023 = process_season_by_month(2022, last_n=7)\n",
    "season_2021_2022 = process_season_by_month(2021, last_n=7)\n",
    "season_2020_2021 = process_season_by_month(2020, last_n=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchups_range1 = process_matchups_for_date_range('2024-10-04', '2024-10-31', last_n=7)\n",
    "matchups_range1.to_csv('../data/g_7-1_10_24.csv', index=False)\n",
    "matchups_range2 = process_matchups_for_date_range('2024-11-01', '2024-11-30', last_n=7)\n",
    "matchups_range2.to_csv('../data/g_7-1_11_24.csv', index=False)\n",
    "matchups_range3 = process_matchups_for_date_range('2024-12-01', '2024-12-31', last_n=7)\n",
    "matchups_range3.to_csv('../data/g_7-1_12_24.csv', index=False)\n",
    "matchups_range4 = process_matchups_for_date_range('2025-01-01', '2025-01-31', last_n=7)\n",
    "matchups_range4.to_csv('../data/g_7-1_01_25.csv', index=False)\n",
    "matchups_range5 = process_matchups_for_date_range('2025-02-01', '2025-02-28', last_n=7)\n",
    "matchups_range5.to_csv('../data/g_7-1_02_25.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "\n",
    "# def analyze_features_for_saves_prediction(df):\n",
    "#     # Remove columns we don't want to consider as features\n",
    "#     exclude_cols = [\n",
    "#         'game_date', 'game_id', 'team', 'goalie_name', 'goalie_team', 'goalie_id', \n",
    "#         'res_decision', 'res_saves', 'res_shots_against', 'res_save_pct', 'res_goals_against'\n",
    "#     ]\n",
    "    \n",
    "#     # Convert boolean columns to int\n",
    "#     df['home'] = df['home'].astype(int)\n",
    "#     df['b2b'] = df['b2b'].astype(int)\n",
    "#     df['opp_b2b'] = df['opp_b2b'].astype(int)\n",
    "    \n",
    "#     # Select numeric columns\n",
    "#     numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "#     feature_cols = [col for col in numeric_cols if col not in exclude_cols]\n",
    "    \n",
    "#     # Create feature matrix and target vector\n",
    "#     X = df[feature_cols].copy()\n",
    "#     y = df['res_saves']\n",
    "    \n",
    "#     # Handle missing values\n",
    "#     X = X.fillna(X.mean())\n",
    "    \n",
    "#     # Scale the features\n",
    "#     scaler = StandardScaler()\n",
    "#     X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "#     # 1. PCA Analysis\n",
    "#     pca = PCA()\n",
    "#     X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "#     # 2. Direct correlation with target\n",
    "#     correlations = []\n",
    "#     for col in feature_cols:\n",
    "#         corr = df[col].corr(y)\n",
    "#         correlations.append((col, abs(corr)))\n",
    "    \n",
    "#     correlation_df = pd.DataFrame(correlations, columns=['Feature', 'Correlation'])\n",
    "#     correlation_df = correlation_df.sort_values('Correlation', ascending=False)\n",
    "    \n",
    "#     # 3. Feature selection using f_regression\n",
    "#     selector = SelectKBest(score_func=f_regression, k='all')\n",
    "#     selector.fit(X_scaled, y)\n",
    "#     f_scores = selector.scores_\n",
    "    \n",
    "#     # Combine all feature importance metrics\n",
    "#     feature_importance_df = pd.DataFrame({\n",
    "#         'Feature': feature_cols,\n",
    "#         'PCA_Importance': np.abs(pca.components_[0]),\n",
    "#         'Correlation': [abs(df[col].corr(y)) for col in feature_cols],\n",
    "#         'F_Score': f_scores\n",
    "#     })\n",
    "    \n",
    "#     # Normalize F-scores\n",
    "#     feature_importance_df['F_Score_Norm'] = feature_importance_df['F_Score'] / feature_importance_df['F_Score'].max()\n",
    "    \n",
    "#     # Calculate combined importance score\n",
    "#     feature_importance_df['Combined_Score'] = (\n",
    "#         feature_importance_df['PCA_Importance'] + \n",
    "#         feature_importance_df['Correlation'] + \n",
    "#         feature_importance_df['F_Score_Norm']\n",
    "#     ) / 3\n",
    "    \n",
    "#     feature_importance_df = feature_importance_df.sort_values('Combined_Score', ascending=False)\n",
    "    \n",
    "#     # Plotting\n",
    "#     plt.figure(figsize=(15, 10))\n",
    "    \n",
    "#     # Plot 1: Top 20 Features by Combined Score\n",
    "#     plt.subplot(2, 1, 1)\n",
    "#     sns.barplot(data=feature_importance_df.head(20), \n",
    "#                 x='Combined_Score', y='Feature')\n",
    "#     plt.title('Top 20 Features by Combined Importance Score')\n",
    "    \n",
    "#     # Plot 2: Correlation Matrix of Top Features with Target\n",
    "#     plt.subplot(2, 1, 2)\n",
    "#     top_features = feature_importance_df.head(10)['Feature'].tolist() + ['res_saves']\n",
    "#     correlation_matrix = df[top_features].corr()\n",
    "#     sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "#     plt.title('Correlation Matrix of Top 10 Features with Saves')\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "    \n",
    "#     # Print detailed analysis\n",
    "#     print(\"\\nTop 20 Most Important Features for Predicting Saves:\")\n",
    "#     print(feature_importance_df[['Feature', 'Combined_Score', 'Correlation', 'PCA_Importance', 'F_Score_Norm']].head(20))\n",
    "    \n",
    "#     # Comprehensive feature grouping\n",
    "#     feature_groups = {\n",
    "#         'Goalie Performance History': [col for col in feature_cols if col.startswith('g_')],\n",
    "#         'Game Context': ['home', 'b2b', 'opp_b2b'],\n",
    "        \n",
    "#         # Team possession metrics\n",
    "#         'Possession Metrics': ['cf', 'ca', 'cf%', 'ff', 'fa', 'ff%'],\n",
    "        \n",
    "#         # Shot metrics\n",
    "#         'Shot Metrics': ['sf', 'sa', 'sf%', 'sh%', 'sv%', 'pdo'],\n",
    "        \n",
    "#         # Goal metrics\n",
    "#         'Goal Metrics': ['gf', 'ga', 'gf%'],\n",
    "        \n",
    "#         # Expected goals metrics\n",
    "#         'Expected Goals': ['xgf', 'xga', 'xgf%'],\n",
    "        \n",
    "#         # Scoring chances metrics\n",
    "#         'Scoring Chances': ['scf', 'sca', 'scf%', 'scsf', 'scsa', 'scsf%', 'scgf', 'scga', 'scgf%', 'scsh%', 'scsv%'],\n",
    "        \n",
    "#         # High-danger chances metrics\n",
    "#         'High-Danger Chances': ['hdcf', 'hdca', 'hdcf%', 'hdsf', 'hdsa', 'hdsf%', 'hdgf', 'hdga', 'hdgf%', 'hdsh%', 'hdsv%'],\n",
    "        \n",
    "#         # Medium-danger chances metrics\n",
    "#         'Medium-Danger Chances': ['mdcf', 'mdca', 'mdcf%', 'mdsf', 'mdsa', 'mdsf%', 'mdgf', 'mdga', 'mdgf%', 'mdsh%', 'mdsv%'],\n",
    "        \n",
    "#         # Low-danger chances metrics\n",
    "#         'Low-Danger Chances': ['ldcf', 'ldca', 'ldcf%', 'ldsf', 'ldsa', 'ldsf%', 'ldgf', 'ldga', 'ldgf%', 'ldsh%', 'ldsv%']\n",
    "#     }\n",
    "    \n",
    "#     # Calculate and print average importance by feature group\n",
    "#     print(\"\\nAverage Importance by Feature Group:\")\n",
    "#     group_importance_data = []\n",
    "    \n",
    "#     for group, features in feature_groups.items():\n",
    "#         valid_features = [f for f in features if f in feature_importance_df['Feature'].values]\n",
    "#         if valid_features:\n",
    "#             group_importance = feature_importance_df[\n",
    "#                 feature_importance_df['Feature'].isin(valid_features)\n",
    "#             ]['Combined_Score'].mean()\n",
    "            \n",
    "#             # Get top feature in this group\n",
    "#             top_feature = feature_importance_df[\n",
    "#                 feature_importance_df['Feature'].isin(valid_features)\n",
    "#             ].iloc[0]['Feature'] if len(valid_features) > 0 else \"None\"\n",
    "            \n",
    "#             # Count features in this group\n",
    "#             feature_count = len(valid_features)\n",
    "            \n",
    "#             group_importance_data.append({\n",
    "#                 'Group': group,\n",
    "#                 'Average_Importance': group_importance,\n",
    "#                 'Feature_Count': feature_count,\n",
    "#                 'Top_Feature': top_feature\n",
    "#             })\n",
    "            \n",
    "#             print(f\"{group}: {group_importance:.4f} (Top: {top_feature}, Count: {feature_count})\")\n",
    "    \n",
    "#     # Create a DataFrame for group importance and plot it\n",
    "#     group_importance_df = pd.DataFrame(group_importance_data)\n",
    "#     group_importance_df = group_importance_df.sort_values('Average_Importance', ascending=False)\n",
    "    \n",
    "#     plt.figure(figsize=(12, 8))\n",
    "#     sns.barplot(data=group_importance_df, x='Average_Importance', y='Group')\n",
    "#     plt.title('Average Feature Importance by Feature Group')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "    \n",
    "#     # Print detailed feature analysis for each group\n",
    "#     print(\"\\nDetailed Feature Analysis by Group:\")\n",
    "#     for group, features in feature_groups.items():\n",
    "#         valid_features = [f for f in features if f in feature_importance_df['Feature'].values]\n",
    "#         if valid_features:\n",
    "#             print(f\"\\n{group}:\")\n",
    "#             group_df = feature_importance_df[feature_importance_df['Feature'].isin(valid_features)]\n",
    "#             group_df = group_df.sort_values('Combined_Score', ascending=False)\n",
    "#             print(group_df[['Feature', 'Combined_Score', 'Correlation']].to_string(index=False))\n",
    "    \n",
    "#     return feature_importance_df\n",
    "\n",
    "# # Run the analysis\n",
    "# feature_importance_df = analyze_features_for_saves_prediction(processed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_game(goalie_history, model, scaler, window_size=10):\n",
    "    \"\"\"Predict save percentage for next game\"\"\"\n",
    "    X, _ = prepare_features(goalie_history, window_size)\n",
    "    X_latest = X.iloc[[-1]]  # Get most recent game's features\n",
    "    X_scaled = scaler.transform(X_latest)\n",
    "    \n",
    "    predicted_sv = model.predict(X_scaled)[0]\n",
    "    return predicted_sv\n",
    "\n",
    "def calculate_performance_scalar(predicted_sv, league_avg_sv=0.910):\n",
    "    \"\"\"Convert predicted save percentage to performance scalar\"\"\"\n",
    "    sv_diff = predicted_sv - league_avg_sv\n",
    "    return 1 - sv_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's evaluate the model\n",
    "processed_data = prepare_game_data(goalie_stats)\n",
    "model, scaler, metrics = train_model(processed_data)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Model Evaluation Metrics:\")\n",
    "print(f\"R² Score: {metrics['r2_score']:.4f}\")\n",
    "print(f\"Mean Absolute Error: {metrics['mae']:.4f}\")\n",
    "print(f\"Root Mean Squared Error: {metrics['rmse']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 6))\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': list(metrics['feature_importance'].keys()),\n",
    "    'importance': list(metrics['feature_importance'].values())\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "sns.barplot(data=importance_df.head(10), x='importance', y='feature')\n",
    "plt.title('Top 10 Most Important Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted values for a specific goalie\n",
    "goalie_name = 'Philipp Grubauer'\n",
    "goalie_data = processed_data[processed_data['player'] == goalie_name].copy()  # Make a copy\n",
    "X, processed_df = prepare_features(goalie_data)\n",
    "X_scaled = scaler.transform(X)\n",
    "predictions = model.predict(X_scaled)\n",
    "\n",
    "# Print dimensions to debug\n",
    "print(f\"Original data length: {len(goalie_data)}\")\n",
    "print(f\"Predictions length: {len(predictions)}\")\n",
    "print(f\"X_scaled shape: {X_scaled.shape}\")\n",
    "\n",
    "# Create the plot with aligned data\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Use the same date range for both actual and predicted values\n",
    "dates = goalie_data['date'].iloc[:-1]  # Remove last date\n",
    "actual_values = goalie_data['sv_pct'].iloc[:-1]  # Remove last actual value\n",
    "\n",
    "plt.plot(dates, actual_values, label='Actual', marker='o')\n",
    "plt.plot(dates, predictions[:-1], label='Predicted Next Game', marker='o')\n",
    "plt.title(f'Actual vs Predicted Save Percentage - {goalie_name}')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Save Percentage')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the prediction for the next game\n",
    "print(f\"\\nPredicted save percentage for {goalie_name}'s next game: {predictions[-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
